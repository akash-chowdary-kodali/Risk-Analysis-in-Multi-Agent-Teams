{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84599692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ File: Human-aware PPO agent vs Human Keyboard Input 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 120\n",
      "- Soups Delivered: 6\n",
      "- Reward Efficiency: 0.30\n",
      "- INTERACTs: Agent 0 = 107, Agent 1 = 34\n",
      "- Idle Steps: Agent 0 = 28, Agent 1 = 293\n",
      "- Action Entropy: Agent 0 = 2.49, Agent 1 = 1.47\n",
      "- Deliveries (approx): Agent 0 = 6, Agent 1 = 12\n",
      "- Coordination Conflict Count: 5\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 40: +20\n",
      "  â€¢ Step 101: +20\n",
      "  â€¢ Step 153: +20\n",
      "  â€¢ Step 239: +20\n",
      "  â€¢ Step 315: +20\n",
      "  â€¢ Step 373: +20\n",
      "\n",
      "ðŸ“„ File: Human-aware PPO agent vs Human-aware PPO agent 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 180\n",
      "- Soups Delivered: 9\n",
      "- Reward Efficiency: 0.45\n",
      "- INTERACTs: Agent 0 = 91, Agent 1 = 96\n",
      "- Idle Steps: Agent 0 = 60, Agent 1 = 72\n",
      "- Action Entropy: Agent 0 = 2.55, Agent 1 = 2.55\n",
      "- Deliveries (approx): Agent 0 = 15, Agent 1 = 12\n",
      "- Coordination Conflict Count: 12\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 38: +20\n",
      "  â€¢ Step 73: +20\n",
      "  â€¢ Step 118: +20\n",
      "  â€¢ Step 157: +20\n",
      "  â€¢ Step 200: +20\n",
      "  â€¢ Step 238: +20\n",
      "  â€¢ Step 273: +20\n",
      "  â€¢ Step 319: +20\n",
      "  â€¢ Step 364: +20\n",
      "\n",
      "ðŸ“„ File: Population Based Training agent vs Human Keyboard Input 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 140\n",
      "- Soups Delivered: 7\n",
      "- Reward Efficiency: 0.35\n",
      "- INTERACTs: Agent 0 = 96, Agent 1 = 41\n",
      "- Idle Steps: Agent 0 = 16, Agent 1 = 251\n",
      "- Action Entropy: Agent 0 = 2.44, Agent 1 = 1.82\n",
      "- Deliveries (approx): Agent 0 = 15, Agent 1 = 6\n",
      "- Coordination Conflict Count: 6\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 67: +20\n",
      "  â€¢ Step 118: +20\n",
      "  â€¢ Step 155: +20\n",
      "  â€¢ Step 223: +20\n",
      "  â€¢ Step 299: +20\n",
      "  â€¢ Step 335: +20\n",
      "  â€¢ Step 386: +20\n",
      "\n",
      "ðŸ“„ File: Population Based Training agent vs Human-aware PPO agent 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 160\n",
      "- Soups Delivered: 8\n",
      "- Reward Efficiency: 0.40\n",
      "- INTERACTs: Agent 0 = 104, Agent 1 = 154\n",
      "- Idle Steps: Agent 0 = 17, Agent 1 = 58\n",
      "- Action Entropy: Agent 0 = 2.43, Agent 1 = 2.34\n",
      "- Deliveries (approx): Agent 0 = 24, Agent 1 = 0\n",
      "- Coordination Conflict Count: 26\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 85: +20\n",
      "  â€¢ Step 157: +20\n",
      "  â€¢ Step 192: +20\n",
      "  â€¢ Step 229: +20\n",
      "  â€¢ Step 263: +20\n",
      "  â€¢ Step 304: +20\n",
      "  â€¢ Step 341: +20\n",
      "  â€¢ Step 385: +20\n",
      "\n",
      "ðŸ“„ File: Population Based Training Agent vs Population Based Training Agent 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 220\n",
      "- Soups Delivered: 11\n",
      "- Reward Efficiency: 0.54\n",
      "- INTERACTs: Agent 0 = 98, Agent 1 = 81\n",
      "- Idle Steps: Agent 0 = 28, Agent 1 = 32\n",
      "- Action Entropy: Agent 0 = 2.38, Agent 1 = 2.50\n",
      "- Deliveries (approx): Agent 0 = 9, Agent 1 = 24\n",
      "- Coordination Conflict Count: 9\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 39: +20\n",
      "  â€¢ Step 74: +20\n",
      "  â€¢ Step 108: +20\n",
      "  â€¢ Step 144: +20\n",
      "  â€¢ Step 179: +20\n",
      "  â€¢ Step 213: +20\n",
      "  â€¢ Step 247: +20\n",
      "  â€¢ Step 281: +20\n",
      "  â€¢ Step 315: +20\n",
      "  â€¢ Step 351: +20\n",
      "  â€¢ Step 385: +20\n",
      "\n",
      "ðŸ“„ File: Population Based Training Agent vs Self-play agent Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 180\n",
      "- Soups Delivered: 9\n",
      "- Reward Efficiency: 0.45\n",
      "- INTERACTs: Agent 0 = 68, Agent 1 = 134\n",
      "- Idle Steps: Agent 0 = 26, Agent 1 = 1\n",
      "- Action Entropy: Agent 0 = 2.43, Agent 1 = 2.22\n",
      "- Deliveries (approx): Agent 0 = 6, Agent 1 = 21\n",
      "- Coordination Conflict Count: 23\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 48: +20\n",
      "  â€¢ Step 85: +20\n",
      "  â€¢ Step 134: +20\n",
      "  â€¢ Step 166: +20\n",
      "  â€¢ Step 209: +20\n",
      "  â€¢ Step 269: +20\n",
      "  â€¢ Step 305: +20\n",
      "  â€¢ Step 344: +20\n",
      "  â€¢ Step 380: +20\n",
      "\n",
      "ðŸ“„ File: Self-play agent vs Human Keyboard Input 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 120\n",
      "- Soups Delivered: 6\n",
      "- Reward Efficiency: 0.30\n",
      "- INTERACTs: Agent 0 = 121, Agent 1 = 30\n",
      "- Idle Steps: Agent 0 = 1, Agent 1 = 292\n",
      "- Action Entropy: Agent 0 = 2.18, Agent 1 = 1.49\n",
      "- Deliveries (approx): Agent 0 = 9, Agent 1 = 9\n",
      "- Coordination Conflict Count: 3\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 61: +20\n",
      "  â€¢ Step 112: +20\n",
      "  â€¢ Step 150: +20\n",
      "  â€¢ Step 194: +20\n",
      "  â€¢ Step 261: +20\n",
      "  â€¢ Step 321: +20\n",
      "\n",
      "ðŸ“„ File: Self-Play agent vs Human-aware PPO agent 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 180\n",
      "- Soups Delivered: 9\n",
      "- Reward Efficiency: 0.45\n",
      "- INTERACTs: Agent 0 = 174, Agent 1 = 108\n",
      "- Idle Steps: Agent 0 = 0, Agent 1 = 71\n",
      "- Action Entropy: Agent 0 = 2.02, Agent 1 = 2.46\n",
      "- Deliveries (approx): Agent 0 = 9, Agent 1 = 18\n",
      "- Coordination Conflict Count: 33\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 38: +20\n",
      "  â€¢ Step 69: +20\n",
      "  â€¢ Step 109: +20\n",
      "  â€¢ Step 150: +20\n",
      "  â€¢ Step 205: +20\n",
      "  â€¢ Step 267: +20\n",
      "  â€¢ Step 319: +20\n",
      "  â€¢ Step 351: +20\n",
      "  â€¢ Step 398: +20\n",
      "\n",
      "ðŸ“„ File: Self-play agent vs Self-play agent 60 sec Cramped Room.json\n",
      "- Episode Length: 404\n",
      "- Total Reward: 200\n",
      "- Soups Delivered: 10\n",
      "- Reward Efficiency: 0.50\n",
      "- INTERACTs: Agent 0 = 171, Agent 1 = 164\n",
      "- Idle Steps: Agent 0 = 1, Agent 1 = 3\n",
      "- Action Entropy: Agent 0 = 2.06, Agent 1 = 2.08\n",
      "- Deliveries (approx): Agent 0 = 18, Agent 1 = 12\n",
      "- Coordination Conflict Count: 89\n",
      "- Avg Reaction Time: 1.0\n",
      "- Reward Events:\n",
      "  â€¢ Step 40: +20\n",
      "  â€¢ Step 71: +20\n",
      "  â€¢ Step 108: +20\n",
      "  â€¢ Step 141: +20\n",
      "  â€¢ Step 203: +20\n",
      "  â€¢ Step 235: +20\n",
      "  â€¢ Step 271: +20\n",
      "  â€¢ Step 306: +20\n",
      "  â€¢ Step 339: +20\n",
      "  â€¢ Step 380: +20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_entropy(action_list):\n",
    "    counts = Counter(str(a) for a in action_list)\n",
    "    total = sum(counts.values())\n",
    "    return -sum((c / total) * math.log(c / total, 2) for c in counts.values() if c > 0)\n",
    "\n",
    "def detect_coordination_conflicts(observations, actions):\n",
    "    conflict_count = 0\n",
    "    for t in range(1, len(observations)):\n",
    "        pos0 = tuple(observations[t][\"players\"][0][\"position\"])\n",
    "        pos1 = tuple(observations[t][\"players\"][1][\"position\"])\n",
    "        act0 = actions[t][0]\n",
    "        act1 = actions[t][1]\n",
    "\n",
    "        both_interact = (\n",
    "            act0 == \"INTERACT\" or (isinstance(act0, list) and \"INTERACT\" in act0)\n",
    "        ) and (\n",
    "            act1 == \"INTERACT\" or (isinstance(act1, list) and \"INTERACT\" in act1)\n",
    "        )\n",
    "\n",
    "        near_same_tile = abs(pos0[0] - pos1[0]) + abs(pos0[1] - pos1[1]) <= 1\n",
    "\n",
    "        if both_interact and near_same_tile:\n",
    "            conflict_count += 1\n",
    "\n",
    "    return conflict_count\n",
    "\n",
    "def analyze_overcooked_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    observations = data['ep_observations'][0]\n",
    "    actions = data['ep_actions'][0]\n",
    "    rewards = data['ep_rewards'][0]\n",
    "\n",
    "    total_reward = sum(rewards)\n",
    "    episode_length = len(observations)\n",
    "    soups_delivered = rewards.count(20)\n",
    "    reward_events = [(i, r) for i, r in enumerate(rewards) if r > 0]\n",
    "\n",
    "    interact_counts = [0, 0]\n",
    "    idle_counts = [0, 0]\n",
    "    action_sequences = [[], []]\n",
    "    division_of_labor = [0, 0]\n",
    "    soup_holder_at_reward = [[], []]\n",
    "    reaction_times = []\n",
    "\n",
    "    for t, frame_actions in enumerate(actions):\n",
    "        for i in [0, 1]:\n",
    "            act = frame_actions[i]\n",
    "            action_sequences[i].append(act)\n",
    "            if act == \"INTERACT\" or (isinstance(act, list) and \"INTERACT\" in act):\n",
    "                interact_counts[i] += 1\n",
    "            elif act == [0, 0]:\n",
    "                idle_counts[i] += 1\n",
    "\n",
    "    for t, r in reward_events:\n",
    "        for dt in range(3):\n",
    "            check_t = max(t - dt, 0)\n",
    "            for agent in [0, 1]:\n",
    "                held = observations[check_t][\"players\"][agent].get(\"held_object\")\n",
    "                if held and held.get(\"name\") == \"soup\":\n",
    "                    soup_holder_at_reward[agent].append(t)\n",
    "                    division_of_labor[agent] += 1\n",
    "                    reaction_times.append(dt)\n",
    "                    break\n",
    "\n",
    "    entropies = [compute_entropy(seq) for seq in action_sequences]\n",
    "    avg_reaction_time = sum(reaction_times) / len(reaction_times) if reaction_times else None\n",
    "    reward_efficiency = total_reward / episode_length if episode_length else 0\n",
    "    conflict_count = detect_coordination_conflicts(observations, actions)\n",
    "\n",
    "    print(f\"\\nðŸ“„ File: {os.path.basename(filepath)}\")\n",
    "    print(f\"- Episode Length: {episode_length}\")\n",
    "    print(f\"- Total Reward: {total_reward}\")\n",
    "    print(f\"- Soups Delivered: {soups_delivered}\")\n",
    "    print(f\"- Reward Efficiency: {reward_efficiency:.2f}\")\n",
    "    print(f\"- INTERACTs: Agent 0 = {interact_counts[0]}, Agent 1 = {interact_counts[1]}\")\n",
    "    print(f\"- Idle Steps: Agent 0 = {idle_counts[0]}, Agent 1 = {idle_counts[1]}\")\n",
    "    print(f\"- Action Entropy: Agent 0 = {entropies[0]:.2f}, Agent 1 = {entropies[1]:.2f}\")\n",
    "    print(f\"- Deliveries (approx): Agent 0 = {division_of_labor[0]}, Agent 1 = {division_of_labor[1]}\")\n",
    "    print(f\"- Coordination Conflict Count: {conflict_count}\")\n",
    "    print(f\"- Avg Reaction Time: {avg_reaction_time if avg_reaction_time else 'N/A'}\")\n",
    "\n",
    "    if reward_events:\n",
    "        print(f\"- Reward Events:\")\n",
    "        for t, r in reward_events:\n",
    "            print(f\"  â€¢ Step {t}: +{r}\")\n",
    "    else:\n",
    "        print(\"- No rewards earned.\")\n",
    "# Correct folder path (change as needed)\n",
    "folder = \"Cramped_room/\"\n",
    "\n",
    "# Run analysis\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        analyze_overcooked_json(os.path.join(folder, filename))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
